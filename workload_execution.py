#!/usr/bin/env python3
"""
PostgreSQL ì›Œí¬ë¡œë“œ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ìƒì„±ëœ ì›Œí¬ë¡œë“œ ì¿¼ë¦¬ë“¤ì„ ì‹¤ì œ PostgreSQLì—ì„œ ì‹¤í–‰í•˜ì—¬ ì„±ê³µë¥ ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
"""

import json
import os
import sys
import time
import re
import signal
from typing import Dict, List, Tuple
import psycopg
from psycopg import sql
import sqlite3
from decimal import Decimal
from datetime import date, datetime

# DB ì—°ê²° ì„¤ì •
DB_CONFIGS = {
    "cordis": {
        "type": "postgresql",
        "url": "postgresql://test:test1234@localhost:5432/cordis",
        "schema": "unics_cordis"
    },
    "oncomx": {
        "type": "postgresql",
        "url": "postgresql://test:test1234@localhost:5432/oncomx", 
        "schema": "oncomx_v1_0_25"
    },
    "sdss": {
        "type": "postgresql",
        "url": "postgresql://test:test1234@localhost:5432/sdss",
        "schema": "lite"
    },
    "eicu": {
        "type": "sqlite",
        "path": "/data/yhyunjun/HybridSQL-Benchmark/EHRSQL/EHRSQL/dataset/ehrsql/eicu/eicu.sqlite"
    },
    "mimic_iii": {
        "type": "sqlite", 
        "path": "/data/yhyunjun/HybridSQL-Benchmark/EHRSQL/EHRSQL/dataset/ehrsql/mimic_iii/mimic_iii.sqlite"
    },
    # BIRD ë°ì´í„°ë² ì´ìŠ¤ë“¤
    "california_schools": {
        "type": "sqlite",
        "path": "/data/yhyunjun/HybridSQL-Benchmark/Bird/dev_20240627/dev_databases/california_schools/california_schools.sqlite"
    },
    "card_games": {
        "type": "sqlite",
        "path": "/data/yhyunjun/HybridSQL-Benchmark/Bird/dev_20240627/dev_databases/card_games/card_games.sqlite"
    },
    "codebase_community": {
        "type": "sqlite",
        "path": "/data/yhyunjun/HybridSQL-Benchmark/Bird/dev_20240627/dev_databases/codebase_community/codebase_community.sqlite"
    },
    "debit_card_specializing": {
        "type": "sqlite",
        "path": "/data/yhyunjun/HybridSQL-Benchmark/Bird/dev_20240627/dev_databases/debit_card_specializing/debit_card_specializing.sqlite"
    },
    "european_football_2": {
        "type": "sqlite",
        "path": "/data/yhyunjun/HybridSQL-Benchmark/Bird/dev_20240627/dev_databases/european_football_2/european_football_2.sqlite"
    },
    "financial": {
        "type": "sqlite",
        "path": "/data/yhyunjun/HybridSQL-Benchmark/Bird/dev_20240627/dev_databases/financial/financial.sqlite"
    },
    "formula_1": {
        "type": "sqlite",
        "path": "/data/yhyunjun/HybridSQL-Benchmark/Bird/dev_20240627/dev_databases/formula_1/formula_1.sqlite"
    },
    "student_club": {
        "type": "sqlite",
        "path": "/data/yhyunjun/HybridSQL-Benchmark/Bird/dev_20240627/dev_databases/student_club/student_club.sqlite"
    },
    "superhero": {
        "type": "sqlite",
        "path": "/data/yhyunjun/HybridSQL-Benchmark/Bird/dev_20240627/dev_databases/superhero/superhero.sqlite"
    },
    "thrombosis_prediction": {
        "type": "sqlite",
        "path": "/data/yhyunjun/HybridSQL-Benchmark/Bird/dev_20240627/dev_databases/thrombosis_prediction/thrombosis_prediction.sqlite"
    },
    "toxicology": {
        "type": "sqlite",
        "path": "/data/yhyunjun/HybridSQL-Benchmark/Bird/dev_20240627/dev_databases/toxicology/toxicology.sqlite"
    }
}

class TimeoutError(Exception):
    """ì¿¼ë¦¬ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ ì˜ˆì™¸"""
    pass

def timeout_handler(signum, frame):
    """íƒ€ì„ì•„ì›ƒ ì‹œê·¸ë„ í•¸ë“¤ëŸ¬"""
    raise TimeoutError("Query execution timeout")

def convert_decimal_to_float(obj):
    """Decimal, date, datetime íƒ€ì…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if isinstance(obj, Decimal):
        return float(obj)
    elif isinstance(obj, date):
        return obj.isoformat()
    elif isinstance(obj, datetime):
        return obj.isoformat()
    elif isinstance(obj, list):
        return [convert_decimal_to_float(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_decimal_to_float(item) for item in obj)
    elif isinstance(obj, dict):
        return {key: convert_decimal_to_float(value) for key, value in obj.items()}
    else:
        return obj

def execute_query_safely(conn, query: str, target_db: str, max_retries: int = 3, timeout_seconds: int = 10) -> Tuple[bool, str, float, List]:
    """
    ì¿¼ë¦¬ë¥¼ ì•ˆì „í•˜ê²Œ ì‹¤í–‰í•©ë‹ˆë‹¤.
    PostgreSQLê³¼ SQLiteë¥¼ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.
    Args:
        conn: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        query: ì‹¤í–‰í•  SQL ì¿¼ë¦¬
        target_db: ëŒ€ìƒ ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        timeout_seconds: ì¿¼ë¦¬ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    Returns: (ì„±ê³µì—¬ë¶€, ì—ëŸ¬ë©”ì‹œì§€, ì‹¤í–‰ì‹œê°„, ì‹¤í–‰ê²°ê³¼)
    """
    start_time = time.time()
    config = DB_CONFIGS[target_db]
    
    for attempt in range(max_retries):
        try:
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •
            old_handler = signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(timeout_seconds)
            
            try:
                if config["type"] == "postgresql":
                    # PostgreSQL: ê° ì¿¼ë¦¬ë¥¼ ë…ë¦½ì ì¸ íŠ¸ëœì­ì…˜ìœ¼ë¡œ ì‹¤í–‰
                    with conn.transaction():
                        with conn.cursor() as cursor:
                            cursor.execute(query)
                            # ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì„œ ë©”ëª¨ë¦¬ì—ì„œ ì²˜ë¦¬ (LIMIT ì ìš©)
                            results = cursor.fetchmany(1000)  # ìµœëŒ€ 1000ê°œë§Œ ê°€ì ¸ì˜¤ê¸°
                            # Decimal íƒ€ì…ì„ floatë¡œ ë³€í™˜
                            results = convert_decimal_to_float(results)
                            execution_time = time.time() - start_time
                            return True, "", execution_time, results
                            
                elif config["type"] == "sqlite":
                    # SQLite: ë‹¨ìˆœ ì‹¤í–‰
                    cursor = conn.cursor()
                    cursor.execute(query)
                    # ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì„œ ë©”ëª¨ë¦¬ì—ì„œ ì²˜ë¦¬ (LIMIT ì ìš©)
                    results = cursor.fetchmany(1000)  # ìµœëŒ€ 1000ê°œë§Œ ê°€ì ¸ì˜¤ê¸°
                    # Decimal íƒ€ì…ì„ floatë¡œ ë³€í™˜
                    results = convert_decimal_to_float(results)
                    cursor.close()
                    execution_time = time.time() - start_time
                    return True, "", execution_time, results
                    
            finally:
                # íƒ€ì„ì•„ì›ƒ í•´ì œ
                signal.alarm(0)
                signal.signal(signal.SIGALRM, old_handler)
                
        except TimeoutError:
            execution_time = time.time() - start_time
            return False, f"Query timeout after {timeout_seconds} seconds", execution_time, []
        except (psycopg.Error, sqlite3.Error) as e:
            error_msg = str(e)
            # PostgreSQL íŠ¸ëœì­ì…˜ ì˜¤ë¥˜ì¸ ê²½ìš° ì¦‰ì‹œ ì¬ì‹œë„í•˜ì§€ ì•Šê³  ë‹¤ìŒ ì¿¼ë¦¬ë¡œ ë„˜ì–´ê°
            if "current transaction is aborted" in error_msg:
                execution_time = time.time() - start_time
                return False, error_msg, execution_time, []
            
            if attempt < max_retries - 1:
                time.sleep(0.1)  # ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„
                continue
            execution_time = time.time() - start_time
            return False, error_msg, execution_time, []
        except Exception as e:
            error_msg = f"Unexpected error: {str(e)}"
            execution_time = time.time() - start_time
            return False, error_msg, execution_time, []
    
    execution_time = time.time() - start_time
    return False, "Max retries exceeded", execution_time, []

def count_masking_tokens(template: str) -> int:
    """í…œí”Œë¦¿ì—ì„œ [m2_x] í˜•íƒœì˜ ê³ ìœ í•œ í† í° ê°œìˆ˜ë¥¼ ì¹´ìš´íŠ¸í•©ë‹ˆë‹¤."""
    if not template:
        return 0
    pattern = r'\[m2_\d+\]'
    tokens = re.findall(pattern, template)
    return len(set(tokens))  # ì¤‘ë³µ ì œê±°í•˜ì—¬ ê³ ìœ í•œ í† í° ê°œìˆ˜ë§Œ ì¹´ìš´íŠ¸

def calculate_masking_distribution(queries: List[Dict]) -> Dict[int, int]:
    """ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ì—ì„œ NLQ ë§ˆìŠ¤í‚¹ ê°œìˆ˜ ë¶„í¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    masking_counts = {}
    
    for query in queries:
        # question_semi_templateì—ì„œ ë§ˆìŠ¤í‚¹ ê°œìˆ˜ ê³„ì‚°
        question_template = query.get('question_semi_template', '')
        if isinstance(question_template, list) and question_template:
            question_template = question_template[0]
        
        masking_count = count_masking_tokens(question_template)
        masking_counts[masking_count] = masking_counts.get(masking_count, 0) + 1
    
    return masking_counts

def get_db_connection(target_db: str):
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    if target_db not in DB_CONFIGS:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ë² ì´ìŠ¤: {target_db}")
    
    config = DB_CONFIGS[target_db]
    if config["type"] == "postgresql":
        return psycopg.connect(config["url"])
    elif config["type"] == "sqlite":
        return sqlite3.connect(config["path"])
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ë² ì´ìŠ¤ íƒ€ì…: {config['type']}")


def test_workload_file(workload_file: str, target_db: str, max_queries: int = None, save_successful_only: bool = False, query_timeout: int = 10, add_execution_data: bool = False) -> Dict:
    """ì›Œí¬ë¡œë“œ íŒŒì¼ì˜ ì¿¼ë¦¬ë“¤ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    print(f"\n{'='*80}")
    print(f"ğŸ§ª ì›Œí¬ë¡œë“œ í…ŒìŠ¤íŠ¸: {os.path.basename(workload_file)}")
    print(f"ğŸ“Š ëŒ€ìƒ DB: {target_db}")
    print(f"â±ï¸ ì¿¼ë¦¬ íƒ€ì„ì•„ì›ƒ: {query_timeout}ì´ˆ")
    print(f"{'='*80}")
    
    # ì›Œí¬ë¡œë“œ íŒŒì¼ ë¡œë“œ
    if not os.path.exists(workload_file):
        print(f"âŒ ì›Œí¬ë¡œë“œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {workload_file}")
        return {"error": "File not found"}
    
    with open(workload_file, 'r', encoding='utf-8') as f:
        workload_data = json.load(f)
    
    queries = workload_data.get("queries", [])
    if max_queries:
        queries = queries[:max_queries]
    
    print(f"ğŸ“ ì´ {len(queries)}ê°œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # DB ì—°ê²°
    try:
        conn = get_db_connection(target_db)
        print(f"âœ… DB ì—°ê²° ì„±ê³µ: {target_db}")
    except Exception as e:
        print(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {e}")
        return {"error": f"DB connection failed: {e}"}
    
    # ì¿¼ë¦¬ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
    results = {
        "total_queries": len(queries),
        "successful_queries": 0,
        "failed_queries": 0,
        "success_rate": 0.0,
        "total_execution_time": 0.0,
        "average_execution_time": 0.0,
        "errors": {},
        "failed_queries_details": [],
        "successful_queries_data": [],  # ì„±ê³µí•œ ì¿¼ë¦¬ ë°ì´í„° ì €ì¥
        "updated_queries": []  # ì‹¤í–‰ ë°ì´í„°ê°€ ì¶”ê°€ëœ ì¿¼ë¦¬ë“¤
    }
    
    start_time = time.time()
    
    for i, query_data in enumerate(queries):
        query_id = query_data.get("id", i + 1)
        sql_query = query_data.get("sql", "")
        question_semi_template = query_data.get('question_semi_template', '')
        # ì¿¼ë¦¬ ë°ì´í„° ë³µì‚¬ (ì›ë³¸ ë³´ì¡´)
        updated_query_data = query_data.copy()
        
        # literal masking ê°œìˆ˜ ê³„ì‚° ë° ì¶”ê°€
        num_literal = count_masking_tokens(question_semi_template)
        updated_query_data["num_literal"] = num_literal
        
        if not sql_query:
            results["failed_queries"] += 1
            results["failed_queries_details"].append({
                "id": query_id,
                "error": "Empty SQL query"
            })
            # ì‹¤í–‰ ë°ì´í„° ì¶”ê°€ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš° ë¹ˆ ê²°ê³¼ë¼ë„ ì¶”ê°€
            if add_execution_data:
                updated_query_data["execution_output"] = []
                results["updated_queries"].append(updated_query_data)
            continue
        
        # ë§ˆìŠ¤í‚¹ í† í°ì´ ë‚¨ì•„ìˆëŠ”ì§€ í™•ì¸
        if re.search(r'\[m[12]_\d+\]', sql_query):
            results["failed_queries"] += 1
            results["failed_queries_details"].append({
                "id": query_id,
                "error": "Unresolved masking token in SQL"
            })
            # ì‹¤í–‰ ë°ì´í„° ì¶”ê°€ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš° ë¹ˆ ê²°ê³¼ë¼ë„ ì¶”ê°€
            if add_execution_data:
                updated_query_data["execution_output"] = []
                results["updated_queries"].append(updated_query_data)
            continue
        
        # ì¿¼ë¦¬ ì‹¤í–‰
        success, error_msg, exec_time, execution_results = execute_query_safely(conn, sql_query, target_db, max_retries=3, timeout_seconds=query_timeout)
        results["total_execution_time"] += exec_time
        
        # ì‹¤í–‰ ê²°ê³¼ë¥¼ ì¿¼ë¦¬ ë°ì´í„°ì— ì¶”ê°€
        if add_execution_data:
            updated_query_data["execution_output"] = execution_results
            results["updated_queries"].append(updated_query_data)
        
        if success:
            results["successful_queries"] += 1
            # ì„±ê³µí•œ ì¿¼ë¦¬ ë°ì´í„° ì €ì¥
            if save_successful_only:
                results["successful_queries_data"].append(updated_query_data)
            if (i + 1) % 50 == 0:
                print(f"âœ… ì§„í–‰ë¥ : {i + 1}/{len(queries)} ({(i + 1)/len(queries)*100:.1f}%)")
        else:
            results["failed_queries"] += 1
            results["failed_queries_details"].append({
                "id": query_id,
                "sql": sql_query[:200] + "..." if len(sql_query) > 200 else sql_query,
                "error": error_msg
            })
            
            # ì—ëŸ¬ íƒ€ì…ë³„ ì¹´ìš´íŠ¸
            error_type = error_msg.split(':')[0] if ':' in error_msg else error_msg
            results["errors"][error_type] = results["errors"].get(error_type, 0) + 1
            
            if results["failed_queries"] <= 10:  # ì²˜ìŒ 10ê°œ ì‹¤íŒ¨ë§Œ ìƒì„¸ ì¶œë ¥
                print(f"âŒ ì¿¼ë¦¬ {query_id} ì‹¤íŒ¨: {error_msg[:100]}...")
    
    # ê²°ê³¼ ê³„ì‚°
    total_time = time.time() - start_time
    results["success_rate"] = (results["successful_queries"] / results["total_queries"]) * 100 if results["total_queries"] > 0 else 0
    results["average_execution_time"] = results["total_execution_time"] / results["total_queries"] if results["total_queries"] > 0 else 0
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*80}")
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*80}")
    print(f"ì´ ì¿¼ë¦¬ ìˆ˜: {results['total_queries']}")
    print(f"ì„±ê³µí•œ ì¿¼ë¦¬: {results['successful_queries']}")
    print(f"ì‹¤íŒ¨í•œ ì¿¼ë¦¬: {results['failed_queries']}")
    print(f"ì„±ê³µë¥ : {results['success_rate']:.1f}%")
    print(f"ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"í‰ê·  ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„: {results['average_execution_time']:.3f}ì´ˆ")
    
    if results["errors"]:
        print(f"\nâŒ ì—ëŸ¬ íƒ€ì…ë³„ í†µê³„:")
        for error_type, count in sorted(results["errors"].items(), key=lambda x: x[1], reverse=True):
            print(f"  - {error_type}: {count}íšŒ")
    
    # ìƒìœ„ 5ê°œ ì‹¤íŒ¨ ì¿¼ë¦¬ ìƒì„¸ ì •ë³´
    if results["failed_queries_details"]:
        print(f"\nğŸ” ì‹¤íŒ¨í•œ ì¿¼ë¦¬ ìƒì„¸ (ìƒìœ„ 5ê°œ):")
        for i, failed_query in enumerate(results["failed_queries_details"][:5]):
            print(f"\n  {i+1}. ì¿¼ë¦¬ ID: {failed_query['id']}")
            if 'sql' in failed_query:
                print(f"     SQL: {failed_query['sql']}")
            print(f"     ì—ëŸ¬: {failed_query['error']}")
    
    conn.close()
    return results

def find_all_workload_files(workloads_dir: str) -> List[Tuple[str, str]]:
    """
    workloads ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ì›Œí¬ë¡œë“œ íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.
    Returns: [(file_path, target_db), ...]
    """
    workload_files = []
    
    if not os.path.exists(workloads_dir):
        print(f"âŒ ì›Œí¬ë¡œë“œ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {workloads_dir}")
        return workload_files
    
    # ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  .json íŒŒì¼ ì°¾ê¸°
    for root, dirs, files in os.walk(workloads_dir):
        for file in files:
            if file.endswith('.json'):
                file_path = os.path.join(root, file)
                
                # íŒŒì¼ ê²½ë¡œì—ì„œ DB ì´ë¦„ ì¶”ì¶œ
                target_db = None
                if "cordis" in file_path:
                    target_db = "cordis"
                elif "oncomx" in file_path:
                    target_db = "oncomx"
                elif "sdss" in file_path:
                    target_db = "sdss"
                elif "eicu" in file_path:
                    target_db = "eicu"
                elif "mimic_iii" in file_path:
                    target_db = "mimic_iii"
                # BIRD ë°ì´í„°ë² ì´ìŠ¤ë“¤
                elif "california_schools" in file_path:
                    target_db = "california_schools"
                elif "card_games" in file_path:
                    target_db = "card_games"
                elif "codebase_community" in file_path:
                    target_db = "codebase_community"
                elif "debit_card_specializing" in file_path:
                    target_db = "debit_card_specializing"
                elif "european_football_2" in file_path:
                    target_db = "european_football_2"
                elif "financial" in file_path:
                    target_db = "financial"
                elif "formula_1" in file_path:
                    target_db = "formula_1"
                elif "student_club" in file_path:
                    target_db = "student_club"
                elif "superhero" in file_path:
                    target_db = "superhero"
                elif "thrombosis_prediction" in file_path:
                    target_db = "thrombosis_prediction"
                elif "toxicology" in file_path:
                    target_db = "toxicology"
                
                if target_db:
                    workload_files.append((file_path, target_db))
                else:
                    print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” DB: {file_path}")
    
    return workload_files



def save_updated_workload(original_file_path: str, target_db: str, updated_queries: List[Dict], output_dir: str):
    """ì‹¤í–‰ ë°ì´í„°ê°€ ì¶”ê°€ëœ ì¿¼ë¦¬ë“¤ë¡œ ìƒˆë¡œìš´ ì›Œí¬ë¡œë“œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    import os
    
    # ì›ë³¸ íŒŒì¼ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    with open(original_file_path, 'r', encoding='utf-8') as f:
        original_data = json.load(f)
    
    # ë§ˆìŠ¤í‚¹ ê°œìˆ˜ ë¶„í¬ ê³„ì‚°
    masking_distribution = calculate_masking_distribution(updated_queries)
    
    
    # ìƒˆë¡œìš´ ì›Œí¬ë¡œë“œ ë°ì´í„° ìƒì„±
    new_workload = {
        "dataset": original_data.get("dataset", target_db),
        "nlq_masking_distribution": masking_distribution,
        "queries": updated_queries,
        "total_queries": len(updated_queries),
        "original_total_queries": original_data.get("total_queries", len(original_data.get("queries", []))),
        "updated_queries_count": len(updated_queries),
        "original_file": os.path.basename(original_file_path),
        "has_execution_data": True,
        "has_literal_count": True
    }
    
    # ì›ë³¸ ë©”íƒ€ë°ì´í„° ë³µì‚¬ (ìˆëŠ” ê²½ìš°)
    for key in ["description", "version", "created_at", "template_info"]:
        if key in original_data:
            new_workload[key] = original_data[key]
    
    # íŒŒì¼ëª… ìƒì„± (ì›ë³¸ê³¼ ë™ì¼í•˜ì§€ë§Œ _updated ì ‘ë¯¸ì‚¬ ì¶”ê°€)
    original_filename = os.path.basename(original_file_path)
    name, ext = os.path.splitext(original_filename)
    new_filename = f"{name}{ext}"
    
    # ì›ë³¸ íŒŒì¼ ê²½ë¡œì—ì„œ ìƒëŒ€ ê²½ë¡œ ì¶”ì¶œ
    workloads_base = "/data/yhyunjun/HybridSQL-Benchmark/workload-construction-2/data/workloads_v3"
    if original_file_path.startswith(workloads_base):
        relative_path = os.path.relpath(original_file_path, workloads_base)
        relative_dir = os.path.dirname(relative_path)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± (ì›ë³¸ê³¼ ë™ì¼)
        output_subdir = os.path.join(output_dir, relative_dir)
        os.makedirs(output_subdir, exist_ok=True)
        
        # íŒŒì¼ ì €ì¥
        output_file_path = os.path.join(output_subdir, new_filename)
    else:
        # ì›ë³¸ ê²½ë¡œê°€ ì˜ˆìƒê³¼ ë‹¤ë¥¸ ê²½ìš° DBë³„ë¡œ ì €ì¥
        db_output_dir = os.path.join(output_dir, target_db)
        os.makedirs(db_output_dir, exist_ok=True)
        output_file_path = os.path.join(db_output_dir, new_filename)
    
    # JSON ì§ë ¬í™”ë¥¼ ìœ„í•´ Decimal íƒ€ì… ë³€í™˜
    new_workload = convert_decimal_to_float(new_workload)
    
    with open(output_file_path, 'w', encoding='utf-8') as f:
        json.dump(new_workload, f, indent=2, ensure_ascii=False)
    
    original_count = new_workload["original_total_queries"]
    updated_count = len(updated_queries)
    
    print(f"ğŸ’¾ ì‹¤í–‰ ë°ì´í„° ì¶”ê°€ëœ ì¿¼ë¦¬ {updated_count}/{original_count}ê°œ ì €ì¥: {output_file_path}")
    
    # ë§ˆìŠ¤í‚¹ ë¶„í¬ í†µê³„ ì¶œë ¥
    if masking_distribution:
        print(f"ğŸ“Š NLQ ë§ˆìŠ¤í‚¹ ê°œìˆ˜ ë¶„í¬:")
        total_queries = sum(masking_distribution.values())
        for masking_count in sorted(masking_distribution.keys()):
            query_count = masking_distribution[masking_count]
            percentage = (query_count / total_queries) * 100 if total_queries > 0 else 0
            print(f"   {masking_count}ê°œ ë§ˆìŠ¤í‚¹: {query_count}ê°œ ì¿¼ë¦¬ ({percentage:.1f}%)")
    
    
    return output_file_path

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ì›Œí¬ë¡œë“œ ì‹¤í–‰ í…ŒìŠ¤íŠ¸')
    parser.add_argument('--max-queries', type=int, default=None, 
                       help='íŒŒì¼ë‹¹ ìµœëŒ€ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìˆ˜ (ê¸°ë³¸ê°’: ì „ì²´)')
    parser.add_argument('--db-filter', type=str, nargs='+', default=None,
                       help='íŠ¹ì • DBë§Œ í…ŒìŠ¤íŠ¸ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥, ì˜ˆ: --db-filter formula_1 student_club codebase_community)')
    parser.add_argument('--exclude-db', type=str, default=None,
                       help='ì œì™¸í•  DB (ì˜ˆ: sdss)')
    parser.add_argument('--file-filter', type=str, default=None,
                       help='íŠ¹ì • íŒŒì¼ íŒ¨í„´ë§Œ í…ŒìŠ¤íŠ¸ (ì˜ˆ: uniform_1k)')
    parser.add_argument('--save-successful', action='store_true',
                       help='ì„±ê³µí•œ ì¿¼ë¦¬ë“¤ë§Œ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥')
    parser.add_argument('--output-dir', type=str, 
                       default='/data/yhyunjun/HybridSQL-Benchmark/workload-construction-2/data/workloads_v3_not_null',
                       help='ì„±ê³µí•œ ì¿¼ë¦¬ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: workloads_v3)')
    parser.add_argument('--query-timeout', type=int, default=10,
                       help='ì¿¼ë¦¬ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ (ì´ˆ, ê¸°ë³¸ê°’: 10)')
    parser.add_argument('--add-execution-data', action='store_true',
                       help='ê° ì¿¼ë¦¬ì— ì‹¤í–‰ ê²°ê³¼ì™€ literal masking ê°œìˆ˜ ì¶”ê°€')
    parser.add_argument('--save-updated', action='store_true',
                       help='ì‹¤í–‰ ë°ì´í„°ê°€ ì¶”ê°€ëœ ì›Œí¬ë¡œë“œ íŒŒì¼ ì €ì¥')
    
    args = parser.parse_args()
    
    print("ğŸš€ PostgreSQL ì›Œí¬ë¡œë“œ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # workloads ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  íŒŒì¼ ì°¾ê¸°
    workloads_dir = "/data/yhyunjun/HybridSQL-Benchmark/workload-construction-2/data/workloads_v3"
    workload_files = find_all_workload_files(workloads_dir)
    
    if not workload_files:
        print("âŒ ì›Œí¬ë¡œë“œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í•„í„°ë§ ì ìš©
    filtered_files = []
    for file_path, target_db in workload_files:
        # DB í•„í„° ì ìš© (ì—¬ëŸ¬ ê°œ ì§€ì›)
        if args.db_filter and target_db not in args.db_filter:
            continue
        
        # DB ì œì™¸ í•„í„° ì ìš©
        if args.exclude_db and target_db == args.exclude_db:
            continue
        
        # íŒŒì¼ íŒ¨í„´ í•„í„° ì ìš©
        if args.file_filter and args.file_filter not in os.path.basename(file_path):
            continue
            
        filtered_files.append((file_path, target_db))
    
    if not filtered_files:
        print("âŒ í•„í„° ì¡°ê±´ì— ë§ëŠ” ì›Œí¬ë¡œë“œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“ ë°œê²¬ëœ ì›Œí¬ë¡œë“œ íŒŒì¼: {len(workload_files)}ê°œ")
    print(f"ğŸ” í•„í„°ë§ í›„: {len(filtered_files)}ê°œ")
    
    for file_path, target_db in filtered_files:
        print(f"  - {os.path.basename(file_path)} ({target_db})")
    
    all_results = {}
    total_start_time = time.time()
    
    for file_path, target_db in filtered_files:
        # ì›Œí¬ë¡œë“œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        result = test_workload_file(file_path, target_db, max_queries=args.max_queries, 
                                  save_successful_only=args.save_successful, query_timeout=args.query_timeout,
                                  add_execution_data=args.add_execution_data)
        

        
        # ì‹¤í–‰ ë°ì´í„°ê°€ ì¶”ê°€ëœ ì¿¼ë¦¬ë“¤ ì €ì¥ (ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°)
        if args.save_updated and "updated_queries" in result and result["updated_queries"]:
            save_updated_workload(file_path, target_db, result["updated_queries"], args.output_dir)
        
        # ê²°ê³¼ë¥¼ íŒŒì¼ë³„ë¡œ ì €ì¥
        file_key = f"{target_db}_{os.path.basename(file_path)}"
        all_results[file_key] = result
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    total_time = time.time() - total_start_time
    print(f"\n{'='*80}")
    print(f"ğŸ¯ ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*80}")
    print(f"ì´ í…ŒìŠ¤íŠ¸ ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"í…ŒìŠ¤íŠ¸ëœ íŒŒì¼ ìˆ˜: {len(all_results)}ê°œ")
    
    # DBë³„ í†µê³„
    db_stats = {}
    total_queries = 0
    total_success = 0
    total_failed = 0
    
    print(f"\nğŸ“Š íŒŒì¼ë³„ ê²°ê³¼:")
    for file_key, result in all_results.items():
        if "error" in result:
            print(f"âŒ {file_key}: {result['error']}")
            continue
            
        total_queries += result["total_queries"]
        total_success += result["successful_queries"] 
        total_failed += result["failed_queries"]
        
        # DBë³„ í†µê³„ ëˆ„ì 
        db_name = file_key.split('_')[0]
        if db_name not in db_stats:
            db_stats[db_name] = {"total": 0, "success": 0, "failed": 0, "files": 0}
        
        db_stats[db_name]["total"] += result["total_queries"]
        db_stats[db_name]["success"] += result["successful_queries"]
        db_stats[db_name]["failed"] += result["failed_queries"]
        db_stats[db_name]["files"] += 1
        
        print(f"ğŸ“„ {file_key}: {result['successful_queries']}/{result['total_queries']} ì„±ê³µ ({result['success_rate']:.1f}%)")
    
    # DBë³„ ìš”ì•½
    print(f"\nğŸ“Š DBë³„ ìš”ì•½:")
    for db_name, stats in db_stats.items():
        success_rate = (stats["success"] / stats["total"]) * 100 if stats["total"] > 0 else 0
        print(f"ğŸ—„ï¸ {db_name.upper()}: {stats['success']}/{stats['total']} ì„±ê³µ ({success_rate:.1f}%) - {stats['files']}ê°œ íŒŒì¼")
    
    # ì „ì²´ ìš”ì•½
    if total_queries > 0:
        overall_success_rate = (total_success / total_queries) * 100
        print(f"\nğŸ† ì „ì²´ ì„±ê³µë¥ : {total_success}/{total_queries} ({overall_success_rate:.1f}%)")
        print(f"ğŸ“ ì´ íŒŒì¼ ìˆ˜: {len(all_results)}ê°œ")
        print(f"â±ï¸ í‰ê·  íŒŒì¼ë‹¹ ì‹œê°„: {total_time/len(all_results):.2f}ì´ˆ")
    
    print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main()

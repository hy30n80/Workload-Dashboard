import json
from pathlib import Path
from collections import Counter

def load_uniform_1k_json(file_path):
    """uniform_rank_1k.json íŒŒì¼ì„ ë¡œë“œí•˜ê³  queriesì™€ statisticsë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data.get('queries', []), data.get('statistics', {})
    except Exception as e:
        print(f"Error loading {file_path}: {e}")
        return None, None

def count_duplicate_sql_queries(queries):
    """queriesì—ì„œ ì •í™•íˆ ë˜‘ê°™ì€ SQL ì¿¼ë¦¬ì˜ ì¤‘ë³µ ê°œìˆ˜ë¥¼ ì²´í¬í•©ë‹ˆë‹¤.
    
    Args:
        queries: ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        tuple: (ì „ì²´ ì¿¼ë¦¬ ê°œìˆ˜, ê³ ìœ  ì¿¼ë¦¬ ê°œìˆ˜, ì¤‘ë³µ ì¿¼ë¦¬ ê°œìˆ˜, ì¤‘ë³µ ì¿¼ë¦¬ ìƒì„¸ ì •ë³´)
    """
    if not queries:
        return 0, 0, 0, {}
    
    # SQL ì¿¼ë¦¬ ë¬¸ìì—´ ì¶”ì¶œ
    sql_queries = []
    for query in queries:
        sql = query.get('sql', '')
        if sql:  # SQLì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
            sql_queries.append(sql)
    
    total_count = len(sql_queries)
    
    # SQL ì¿¼ë¦¬ë³„ ê°œìˆ˜ ì§‘ê³„
    sql_counter = Counter(sql_queries)
    
    # ì¤‘ë³µ ì¿¼ë¦¬ ì°¾ê¸° (2ê°œ ì´ìƒì¸ ê²ƒë“¤)
    duplicate_queries = {sql: count for sql, count in sql_counter.items() if count > 1}
    
    unique_count = len(sql_counter)
    duplicate_count = total_count - unique_count  # ì¤‘ë³µëœ ì¿¼ë¦¬ì˜ ì´ ê°œìˆ˜
    
    return total_count, unique_count, duplicate_count, duplicate_queries

def main(): 
    base_dir = Path("/data/yhyunjun/HybridSQL-Benchmark/workload-construction-2/data/workloads_v15_1k/Dev")
    
    datasets = ["BIRD", "EHRSQL", "ScienceBenchmark"]
    
    # ì „ì²´ í†µê³„
    total_stats = {
        'total_workloads': 0,
        'total_queries': 0,
        'total_unique_queries': 0,
        'total_duplicate_queries': 0,
        'workloads_with_duplicates': 0
    }
    
    print("=" * 80)
    print("SQL Query ì¤‘ë³µ ì²´í¬ ë¦¬í¬íŠ¸")
    print("=" * 80)
    
    for dataset in datasets:
        dataset_path = base_dir / dataset
        if not dataset_path.exists():
            print(f"\nâš ï¸  Warning: {dataset_path} does not exist. Skipping...")
            continue
        
        print(f"\nğŸ”¹ Processing {dataset}...")
        print("-" * 80)
        
        dataset_total = 0
        dataset_unique = 0
        dataset_duplicate = 0
        dataset_workloads_with_dups = 0
        
        # ê° DB ë””ë ‰í† ë¦¬ íƒìƒ‰
        for db_dir in sorted(dataset_path.iterdir()):
            if not db_dir.is_dir():
                continue
            
            db_name = db_dir.name
            uniform_file = db_dir / "uniform_rank_1k.json"
            
            if not uniform_file.exists():
                print(f"  âš ï¸  {db_name}: uniform_rank_1k.json not found. Skipping...")
                continue
            
            # JSON íŒŒì¼ ë¡œë“œ
            queries, stats = load_uniform_1k_json(uniform_file)
            if queries is None:
                continue
            
            # ì¤‘ë³µ ì¿¼ë¦¬ ì²´í¬
            total_count, unique_count, duplicate_count, duplicate_queries = count_duplicate_sql_queries(queries)
            
            dataset_total += total_count
            dataset_unique += unique_count
            dataset_duplicate += duplicate_count
            
            total_stats['total_workloads'] += 1
            total_stats['total_queries'] += total_count
            total_stats['total_unique_queries'] += unique_count
            total_stats['total_duplicate_queries'] += duplicate_count
            
            # ê²°ê³¼ ì¶œë ¥
            if duplicate_count > 0:
                dataset_workloads_with_dups += 1
                total_stats['workloads_with_duplicates'] += 1
                print(f"  ğŸ”´ {db_name}:")
                print(f"     ì „ì²´ ì¿¼ë¦¬: {total_count}, ê³ ìœ  ì¿¼ë¦¬: {unique_count}, ì¤‘ë³µ ì¿¼ë¦¬: {duplicate_count}")
                print(f"     ì¤‘ë³µ ë¹„ìœ¨: {duplicate_count/total_count*100:.2f}%")
                
                # ê°€ì¥ ë§ì´ ì¤‘ë³µëœ ì¿¼ë¦¬ ìƒìœ„ 5ê°œ ì¶œë ¥
                if duplicate_queries:
                    sorted_dups = sorted(duplicate_queries.items(), key=lambda x: x[1], reverse=True)
                    print(f"     ì¤‘ë³µ ì¿¼ë¦¬ ìƒìœ„ 5ê°œ:")
                    for idx, (sql, count) in enumerate(sorted_dups[:5], 1):
                        sql_preview = sql[:80] + "..." if len(sql) > 80 else sql
                        print(f"       {idx}. (ì¤‘ë³µ {count}íšŒ) {sql_preview}")
            else:
                print(f"  âœ… {db_name}: ì¤‘ë³µ ì—†ìŒ (ì „ì²´: {total_count}, ê³ ìœ : {unique_count})")
        
        # ë°ì´í„°ì…‹ë³„ ìš”ì•½
        print(f"\nğŸ“Š {dataset} ìš”ì•½:")
        print(f"   ì „ì²´ ì¿¼ë¦¬: {dataset_total}, ê³ ìœ  ì¿¼ë¦¬: {dataset_unique}, ì¤‘ë³µ ì¿¼ë¦¬: {dataset_duplicate}")
        if dataset_total > 0:
            print(f"   ì¤‘ë³µ ë¹„ìœ¨: {dataset_duplicate/dataset_total*100:.2f}%")
        print(f"   ì¤‘ë³µì´ ìˆëŠ” ì›Œí¬ë¡œë“œ: {dataset_workloads_with_dups}")
    
    # ì „ì²´ ìš”ì•½
    print("\n" + "=" * 80)
    print("ì „ì²´ ìš”ì•½")
    print("=" * 80)
    print(f"ì „ì²´ ì›Œí¬ë¡œë“œ ìˆ˜: {total_stats['total_workloads']}")
    print(f"ì „ì²´ ì¿¼ë¦¬ ìˆ˜: {total_stats['total_queries']}")
    print(f"ì „ì²´ ê³ ìœ  ì¿¼ë¦¬ ìˆ˜: {total_stats['total_unique_queries']}")
    print(f"ì „ì²´ ì¤‘ë³µ ì¿¼ë¦¬ ìˆ˜: {total_stats['total_duplicate_queries']}")
    if total_stats['total_queries'] > 0:
        print(f"ì „ì²´ ì¤‘ë³µ ë¹„ìœ¨: {total_stats['total_duplicate_queries']/total_stats['total_queries']*100:.2f}%")
    print(f"ì¤‘ë³µì´ ìˆëŠ” ì›Œí¬ë¡œë“œ ìˆ˜: {total_stats['workloads_with_duplicates']}")
    if total_stats['total_workloads'] > 0:
        print(f"ì¤‘ë³µ ì›Œí¬ë¡œë“œ ë¹„ìœ¨: {total_stats['workloads_with_duplicates']/total_stats['total_workloads']*100:.2f}%")
    print("=" * 80)

if __name__ == "__main__":
    main()

